{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing your code\n",
    "## Introduction\n",
    "Like all code, llm code needs testing. But how does it work? It turns out , it's not that different from regular testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain langchain-openai langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple testing - Trial and Error\n",
    "We start by asking a simple question to openAI to find out where Patrick lives.\n",
    "In our test, we want to check for the word Ghent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update, specific and current personal residence details of individuals such as Patrick Debois, who is known for his work in the DevOps community, are not typically disclosed for privacy and security reasons. If you're interested in his professional work or public contributions, I can provide information on his role in the development of DevOps or his public engagements.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def ask_model(model, question):\n",
    "    llm = ChatOpenAI(model=model, temperature=0)\n",
    "    completion = llm.invoke(question)\n",
    "    return completion.content\n",
    "\n",
    "answer = ask_model(\"gpt-4-turbo\", \"Where does Patrick Debois live ?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too bad , OpenAI doesn't give an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different model from OpenAI and see if that works better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I can't provide personal information about individuals, including their addresses or places of residence.\n"
     ]
    }
   ],
   "source": [
    "answer = ask_model(\"gpt-4o-mini\", \"Where does Patrick Debois live ?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no luck, let's see what Anthropic knows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I do not have enough accurate information to confidently state the city where Patrick Debois currently lives. Patrick Debois is a Belgian software engineer and consultant who is known for coining the term \"DevOps\". However, his exact city of residence is not widely publicized or something I can confirm with high confidence based on the information available to me.\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "def ask_model_anthropic(model, question):\n",
    "    llm = ChatAnthropic(model=model, temperature=0)  # note the temperature=0 to get deterministic results\n",
    "    completion = llm.invoke(question)\n",
    "    return completion.content\n",
    "\n",
    "answer = ask_model_anthropic(\"claude-3-opus-20240229\", \"In what city does Patrick Debois live ?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no luck, maybe let's try to ask the question in a slightly different phrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patrick Debois is a Belgian consultant, project manager and agile practitioner who is credited with coining the term \"DevOps\". He currently resides in Ghent, Belgium.\n"
     ]
    }
   ],
   "source": [
    "answer = ask_model_anthropic(\"claude-3-opus-20240229\", \"Where in the world does Patrick Debois live ?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we got the answer we wanted. As we just had a string returned, we can check if the string contained the right answer. We can use a regex , or check the length of a string etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact testing\n",
    "Validating if the word Ghent appears in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(\"Ghent\" in answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can also check for the length the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 5\n",
      "In the realm where code and operation blend,\n",
      "DevOps dances, a seamless, flowing trend.\n",
      "Through pipelines that automate each deploy,\n",
      "Ensuring uptime, users joyfully enjoy.\n",
      "A symphony of scripts and tools, endlessly extend.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def ask_model(model, question):\n",
    "    llm = ChatOpenAI(model=model, temperature=0) # note the temperature=0 to get deterministic results\n",
    "    completion = llm.invoke(question)\n",
    "    return completion.content\n",
    "\n",
    "answer = ask_model(\"gpt-4-turbo\", \"Write me poem about DevOps in 5 lines\")\n",
    "\n",
    "number_of_lines = len(answer.splitlines())\n",
    "print(f\"Number of lines: {number_of_lines}\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've seen that different prompts, different models, even changing a few words have an impact on the result.\n",
    "That's why you need a test framework, even it's as simple as checking if the answer contains certain text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01-llm-and-prompting",
   "language": "python",
   "name": "01-llm-and-prompting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
