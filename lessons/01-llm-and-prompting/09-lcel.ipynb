{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain - LCEL\n",
    "## Introduction\n",
    "\n",
    "LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\n",
    "It is similar to the Unix pipe methodology to easily combine different smaller code parts and making it easy to replace parts of the *chain*.\n",
    "\n",
    "See <https://python.langchain.com/v0.1/docs/expression_language/> for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An LCEL chain example with OpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple example of creating a chain to generate jokes: it consists of a prompt, model and output_parser. And now that chain can be use on its own.\n",
    "\n",
    "Example taken from <https://python.langchain.com/v0.1/docs/expression_language/primitives/sequence/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the ice cream truck break down?\n",
      "\n",
      "Because it had a rocky road!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# The chat prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "# The model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
    "\n",
    "# A parser that just outputs the text\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Notice here how we can pipe the prompt to the model and then to the output parser\n",
    "# similar to unix pipes\n",
    "chain = prompt | model | output_parser\n",
    "result = chain.invoke({\"topic\": \"ice cream\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another chain to evaluate the previous chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build a set of two chains (in Anthropic): one to generate the joke, another to check the joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "model = ChatAnthropic(model_name=\"claude-3-haiku-20240307\",temperature=0)\n",
    "\n",
    "joke_chain = joke_prompt | model | StrOutputParser()\n",
    "joke_chain.invoke({\"topic\": \"bears\"})\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put it all together in one single chain both generating and judging the joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a pretty good pun-based joke! The humor comes from the play on words with \"root\" relationship, which refers to the fact that beets and carrots are both root vegetables.\n",
      "\n",
      "The joke works because:\n",
      "\n",
      "1. It takes a common relationship concept (breaking up) and applies it to two vegetables, which is an unexpected and slightly absurd pairing.\n",
      "\n",
      "2. The pun on \"root\" relationship is clever and makes the joke more than just a simple vegetable-based joke. It adds an extra layer of wordplay.\n",
      "\n",
      "3. Puns and plays on words are a common source of humor, as they surprise the listener and make them think about the double meaning.\n",
      "\n",
      "So in this case, the combination of the unexpected subject matter, the clever pun, and the overall silliness of the premise makes it a fairly amusing and well-constructed vegetable-themed joke. Puns like this can be hit-or-miss, but this one lands pretty well.\n"
     ]
    }
   ],
   "source": [
    "# A chain that both generates the joke and analyses the joke\n",
    "composed_chain = {\"joke\": chain} | analysis_prompt | model | StrOutputParser()\n",
    "composed_chain.invoke({\"topic\": \"bears\"})\n",
    "\n",
    "composed_chain_with_lambda = (\n",
    "    chain\n",
    "    | (lambda input: {\"joke\": input})\n",
    "    | analysis_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "analyzed_joke_result = composed_chain_with_lambda.invoke({\"topic\": \"beets\"})\n",
    "print(analyzed_joke_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of the LCEL syntax is that is drives resusability of chains similar to Unix commands."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01-llm-and-prompting",
   "language": "python",
   "name": "01-llm-and-prompting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
