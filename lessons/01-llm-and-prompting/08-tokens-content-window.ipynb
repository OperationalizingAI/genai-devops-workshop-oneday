{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens and Context Windows\n",
    "## Introduction\n",
    "As we've seen in the streaming example, llms don't operate in words but in tokens.\n",
    "\n",
    "Let's explore what tokens actually are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Size\n",
    "\n",
    "- Each model has a maximum of tokens that can be send. This is the `context_size` of a model. \n",
    "- Billing of requests are also done based on the number of tokens sent\n",
    "- Refer to the documentation of the models to know what this limit is\n",
    "\n",
    "It's useful to understand how many tokens you send. Here's a simple way to count it in langchain.\n",
    "As you can see the count of tokens is not exact the same as the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
    "prompt = \"Where in the world is DevOps Waldo ?\"\n",
    "nr_of_tokens = llm.get_num_tokens(prompt)\n",
    "print(nr_of_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token encoder/decoder\n",
    "The translation between text and tokens is done through an `encoder`. Different models use different encoders.\n",
    "\n",
    "To handle tokens , encoders and decoders we can use the popular library `tiktoken` : a fast open-source tokenizer by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the different encoding models for each OpenAI model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'cl100k_base'>\n",
      "<Encoding 'o200k_base'>\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "print(encoding)\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for example Anthropic has not publicly described their tokenizer model, but there are community efforts to reverse engineer it <https://github.com/javirandor/anthropic-tokenizer>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9241, 304, 279, 1917, 374, 6168, 40004, 14916, 3055, 949]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens = encoding.encode(\"Where in the world is DevOps Waldo ?\")\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the text as tokens : where each token begin a unique number.\n",
    "\n",
    "## Decoding tokens\n",
    "Now we can decode it back to text again. Here we convert it to bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Where',\n",
       " b' in',\n",
       " b' the',\n",
       " b' world',\n",
       " b' is',\n",
       " b' Dev',\n",
       " b'Ops',\n",
       " b' Wal',\n",
       " b'do',\n",
       " b' ?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoding.decode_single_token_bytes(token) for token in tokens]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01-llm-and-prompting",
   "language": "python",
   "name": "01-llm-and-prompting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
